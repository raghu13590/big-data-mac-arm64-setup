services:
  spark-master:
    image: spark-local
    container_name: spark-master
    ports:
      - "7077:7077"
      - "8082:8082"
    environment:
      - SPARK_MASTER_HOST=spark-master
      - SPARK_PUBLIC_DNS=localhost
      - SPARK_MASTER_WEBUI_PORT=8082
      - HADOOP_CONF_DIR=/opt/hadoop-3.3.6/etc/hadoop
    volumes:
      - ../configs/spark/spark-env.sh:/opt/spark/conf/spark-env.sh
      - ../configs/spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ../configs/hadoop/core-site.xml:/opt/hadoop-3.3.6/etc/hadoop/core-site.xml
      - ../configs/hadoop/hdfs-site.xml:/opt/hadoop-3.3.6/etc/hadoop/hdfs-site.xml
      - ../configs/hadoop/yarn-site.xml:/opt/hadoop-3.3.6/etc/hadoop/yarn-site.xml
      - ../configs/hadoop/hive-site.xml:/opt/hive/hive-site.xml
      - ../app-data/spark/history:/opt/spark/history
      - ../app-data/spark/datasets:/opt/spark/datasets
      - ../app-data/spark/local-jars:/opt/spark/local-jars
    networks:
      - big-data-network
    command: ["spark-class", "org.apache.spark.deploy.master.Master", "--webui-port", "8082"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8082"]
      interval: 30s
      timeout: 10s
      retries: 5

  spark-worker-1:
    image: spark-local
    container_name: spark-worker-1
    environment:
      - SPARK_WORKER_WEBUI_PORT=8083
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_PUBLIC_DNS=localhost
      - HADOOP_CONF_DIR=/opt/hadoop-3.3.6/etc/hadoop
    ports:
      - "8083:8083"
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - ../configs/spark/spark-env.sh:/opt/spark/conf/spark-env.sh
      - ../configs/spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ../configs/hadoop/core-site.xml:/opt/hadoop-3.3.6/etc/hadoop/core-site.xml
      - ../configs/hadoop/hdfs-site.xml:/opt/hadoop-3.3.6/etc/hadoop/hdfs-site.xml
      - ../configs/hadoop/yarn-site.xml:/opt/hadoop-3.3.6/etc/hadoop/yarn-site.xml
      - ../configs/hadoop/hive-site.xml:/opt/hive/hive-site.xml
      - ../app-data/spark/history:/opt/spark/history
      - ../app-data/spark/datasets:/opt/spark/datasets
      - ../app-data/spark/local-jars:/opt/spark/local-jars
    networks:
      - big-data-network
    command: ["spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077", "--webui-port", "8083"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8083"]
      interval: 30s
      timeout: 10s
      retries: 5

  spark-worker-2:
    image: spark-local
    container_name: spark-worker-2
    environment:
      - SPARK_WORKER_WEBUI_PORT=8084
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_PUBLIC_DNS=localhost
      - HADOOP_CONF_DIR=/opt/hadoop-3.3.6/etc/hadoop
    ports:
      - "8084:8084"
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - ../configs/spark/spark-env.sh:/opt/spark/conf/spark-env.sh
      - ../configs/spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ../configs/hadoop/core-site.xml:/opt/hadoop-3.3.6/etc/hadoop/core-site.xml
      - ../configs/hadoop/hdfs-site.xml:/opt/hadoop-3.3.6/etc/hadoop/hdfs-site.xml
      - ../configs/hadoop/yarn-site.xml:/opt/hadoop-3.3.6/etc/hadoop/yarn-site.xml
      - ../configs/hadoop/hive-site.xml:/opt/hive/hive-site.xml
      - ../app-data/spark/history:/opt/spark/history
      - ../app-data/spark/datasets:/opt/spark/datasets
      - ../app-data/spark/local-jars:/opt/spark/local-jars
    networks:
      - big-data-network
    command: ["spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077", "--webui-port", "8084"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8084"]
      interval: 30s
      timeout: 10s
      retries: 5

  spark-history:
    image: spark-local
    container_name: spark-history
    ports:
      - "18080:18080"
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - ../configs/spark/spark-env.sh:/opt/spark/conf/spark-env.sh
      - ../configs/spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ../app-data/spark/history:/opt/spark/history
    networks:
      - big-data-network
    command: ["spark-class", "org.apache.spark.deploy.history.HistoryServer"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:18080"]
      interval: 30s
      timeout: 10s
      retries: 5

networks:
  big-data-network:
    external: true
x-build-default: &build-default
  build:
    context: ${BUILD_CONTEXT}

x-healthcheck-default: &healthcheck-default
  interval: ${HEALTHCHECK_INTERVAL}
  timeout: ${HEALTHCHECK_TIMEOUT}
  retries: ${HEALTHCHECK_RETRIES}

services:
  spark-master:
    <<: *build-default
    container_name: spark-master
    ports:
      - "7077:7077"
      - "8072:8072"
      - "5005:5005"
      - "4040:4040"
    environment:
      - SPARK_MASTER_HOST=spark-master
      - SPARK_PUBLIC_DNS=localhost
      - SPARK_MASTER_WEBUI_PORT=8072
      - HADOOP_CONF_DIR=/opt/hadoop-3.3.6/etc/hadoop
      - ENABLE_DEBUG=true
    volumes:
      - ${CONFIGS_DIR}/spark-env.sh:/opt/spark/conf/spark-env.sh
      - ${CONFIGS_DIR}/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ../configs/hadoop/core-site.xml:/opt/hadoop-3.3.6/etc/hadoop/core-site.xml
      - ../configs/hadoop/hdfs-site.xml:/opt/hadoop-3.3.6/etc/hadoop/hdfs-site.xml
      - ../configs/hadoop/yarn-site.xml:/opt/hadoop-3.3.6/etc/hadoop/yarn-site.xml
      - ../configs/hadoop/hive-site.xml:/opt/hive/hive-site.xml
      - ${APP_DATA_DIR}/history:/opt/spark/history
      - ${APP_DATA_DIR}/datasets:/opt/spark/datasets
      - ${APP_DATA_DIR}/local-jars:/opt/spark/local-jars
    networks:
      - ${BIG_DATA_NETWORK}
    command: ["spark-class", "org.apache.spark.deploy.master.Master", "--webui-port", "8072"]
    healthcheck:
      <<: *healthcheck-default
      test: ["CMD", "curl", "-f", "http://localhost:8072"]
      

  spark-worker-1:
    <<: *build-default
    container_name: spark-worker-1
    environment:
      - SPARK_WORKER_WEBUI_PORT=8073
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=4
      - SPARK_PUBLIC_DNS=localhost
      - HADOOP_CONF_DIR=/opt/hadoop-3.3.6/etc/hadoop
      - ENABLE_DEBUG=true
    ports:
      - "8073:8073"
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - ${CONFIGS_DIR}/spark-env.sh:/opt/spark/conf/spark-env.sh
      - ${CONFIGS_DIR}/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ../configs/hadoop/core-site.xml:/opt/hadoop-3.3.6/etc/hadoop/core-site.xml
      - ../configs/hadoop/hdfs-site.xml:/opt/hadoop-3.3.6/etc/hadoop/hdfs-site.xml
      - ../configs/hadoop/yarn-site.xml:/opt/hadoop-3.3.6/etc/hadoop/yarn-site.xml
      - ../configs/hadoop/hive-site.xml:/opt/hive/hive-site.xml
      - ${APP_DATA_DIR}/history:/opt/spark/history
      - ${APP_DATA_DIR}/datasets:/opt/spark/datasets
      - ${APP_DATA_DIR}/local-jars:/opt/spark/local-jars
    networks:
      - ${BIG_DATA_NETWORK}
    command: ["spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077", "--webui-port", "8073"]
    healthcheck:
      <<: *healthcheck-default
      test: ["CMD", "curl", "-f", "http://localhost:8073"]

  spark-worker-2:
    <<: *build-default
    container_name: spark-worker-2
    environment:
      - SPARK_WORKER_WEBUI_PORT=8075
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=4
      - SPARK_PUBLIC_DNS=localhost
      - HADOOP_CONF_DIR=/opt/hadoop-3.3.6/etc/hadoop
      - ENABLE_DEBUG=true
    ports:
      - "8075:8075"
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - ${CONFIGS_DIR}/spark-env.sh:/opt/spark/conf/spark-env.sh
      - ${CONFIGS_DIR}/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ../configs/hadoop/core-site.xml:/opt/hadoop-3.3.6/etc/hadoop/core-site.xml
      - ../configs/hadoop/hdfs-site.xml:/opt/hadoop-3.3.6/etc/hadoop/hdfs-site.xml
      - ../configs/hadoop/yarn-site.xml:/opt/hadoop-3.3.6/etc/hadoop/yarn-site.xml
      - ../configs/hadoop/hive-site.xml:/opt/hive/hive-site.xml
      - ${APP_DATA_DIR}/history:/opt/spark/history
      - ${APP_DATA_DIR}/datasets:/opt/spark/datasets
      - ${APP_DATA_DIR}/local-jars:/opt/spark/local-jars
    networks:
      - ${BIG_DATA_NETWORK}
    command: ["spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077", "--webui-port", "8075"]
    healthcheck:
      <<: *healthcheck-default
      test: ["CMD", "curl", "-f", "http://localhost:8075"]
      

  spark-history:
    <<: *build-default
    container_name: spark-history
    ports:
      - "18080:18080"
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - ${CONFIGS_DIR}/spark-env.sh:/opt/spark/conf/spark-env.sh
      - ${CONFIGS_DIR}/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ${APP_DATA_DIR}/history:/opt/spark/history
    networks:
      - ${BIG_DATA_NETWORK}
    command: ["spark-class", "org.apache.spark.deploy.history.HistoryServer"]
    healthcheck:
      <<: *healthcheck-default
      test: ["CMD", "curl", "-f", "http://localhost:18080"]
      